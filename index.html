<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Venkata Siva Reddy Naga</title>
  <meta name="description" content="Portfolio of Venkata Siva Reddy Naga — ML/DS/DE/SDE projects." />
  <link rel="stylesheet" href="/assets/css/style.css" />
</head>

<body>
  <header class="container" id="site-header"></header>
  <script src="/assets/js/header.js"></script>
  <script src="/assets/js/theme.js"></script>

  <main class="container">
    <section class="hero">
      <h1>Venkata Naga</h1>
      <p class="sub">
        M.S. Applied Data Science @ University of Florida • ML / Data / Software Engineering
      </p>

      <div class="cta">
        <a class="btn" href="/projects/">View Projects</a>
        <a class="btn ghost" href="https://github.com/venkatasivanaga" target="_blank" rel="noreferrer">GitHub</a>
        <!-- Optional: add LinkedIn + Resume PDF later -->
      </div>
    </section>

    <section id="skills" class="card">
      <h2>Skills</h2>
      <div class="grid">
        <div>
          <h3>Programming</h3>
          <p>Python, Java, SQL, R, Linux/Bash, Object-Oriented Programming (OOP)</p>
        </div>
        <div>
          <h3>Machine Learning & Deep Learning</h3>
          <p>Anomaly detection, clustering, time series analysis, CNNs, U-Net, LSTM, classification, regression, ROC-AUC, F1, precision/recall</p>
        </div>
        <div>
          <h3>Backend & APIs</h3>
          <p>REST APIs, HTTP/HTTPS, JSON, FastAPI, Authentication (JWT), API documentation</p>
        </div>
        <div>
          <h3>MLOps & DevOps</h3>
          <p>Git/GitHub, Docker, MLflow, Jupyter, VS Code, Profiling & performance: torch.profiler, nvidia-smi</p>
        </div>
        <div>
          <h3>Cloud</h3>
          <p>AWS: EC2, S3, Lambda, API Gateway, Azure</p>
        </div>
        <div>
          <h3>BI Tools</h3>
          <p>Tableau, Power BI, Excel (Advanced)</p>
        </div>
        <div>
          <h3>Databases</h3>
          <p>PostgreSQL, MySQL, Oracle/PL/SQL, MongoDB, BigQuery, Snowflake</p>
        </div>
      </div>
    </section>

    <section id="education" class="card">
      <h2>Education</h2>

      <div class="timeline">
        <div class="row">
          <div>
            <div class="title">University of Florida</div>
            <div class="meta">M.S. in Applied Data Science • 2024 -- Expected May 2026</div>
            <div class="sub">Relevant CourseWork: Machine Learning, Medical Imaging using Deep Learning, Statistics and Probability, Data Engineering, Data Science</div>
          </div>
          <div class="right">Gainesville, FL</div>
        </div>

        <!-- Optional: add your bachelor’s below -->
      
        <div class="row">
          <div>
            <div class="title">R.M.K. Engineering College</div>
            <div class="meta">B.Tech in Artificial Intelligence & Data Science • 2020 -- 2024</div>
            <div class="sub">Relevant CourseWork: Data Science, Software Engineering, Artificial Intelligence, High-Performance Computing, Linear Algebra </div>
          </div>
          <div class="right">Chennai, India</div>
        </div>
    
      </div>
    </section>


    <section id="experience" class="card">
  <h2>Experience</h2>

  <div class="timeline">
    <div class="row">
      <div>
        <div class="title">Research Assistant • Forest Biometrics & Remote Sensing Lab</div>
        <div class="meta">University of Florida • May 2025 – Present</div>
        <div class="sub">
          Built reproducible ML pipelines for LiDAR point clouds + remote sensing data, training advanced PointNet-based models for canopy/fuels analysis with strong evaluation and iteration workflow.
          </div>

            <h4 class="mini">What I did</h4>
            <ul class="bullets">
              <li>Designed an end-to-end pipeline for preprocessing → patch generation → training → evaluation on LiDAR point-cloud datasets.</li>
              <li>Trained advanced PointNet-based architectures (PointNet++ style) and compared experiments using repeatable configs and tracked metrics.</li>
              <li>Streamlined data handling and training runs to support repeatable iteration and reliable results across multiple experiments.</li>
            </ul>

            <h4 class="mini">Scale & results</h4>
            <ul class="bullets">
              <li>Processed <b>7–10 GB</b> of LiDAR/remote-sensing data and generated <b>100–1000</b> training patches for modeling.</li>
              <li>Completed training/evaluation runs spanning <b>~4–19 hours</b> depending on dataset size and model settings.</li>
              <li>Produced project-ready outputs: cleaned datasets, experiment logs, metrics summaries, and qualitative visual results.</li>
            </ul>

            <div class="tech">
              <span class="tag">Python</span>
              <span class="tag">PyTorch</span>
              <span class="tag">PointNet++</span>
              <span class="tag">LiDAR</span>
              <span class="tag">Remote Sensing</span>
              <span class="tag">Linux</span>
              <span class="tag">Git</span>
            </div>
     
      </div>
      <div class="right">Gainesville, FL</div>
    </div>

    <!-- Add more roles similarly -->
    
    <div class="row">
      <div>
        <div class="title">Software Engineer Intern • VNR IT Solutions Pvt.Ltd. </div>
        <div class="meta">Nov 2023 – Jun 2024</div>
        <div class="sub">
          Supported cross-functional delivery by gathering requirements, shaping feature documentation, and managing execution artifacts (plans, risks, and release checklists) to keep work on track.
        </div>

        <h4 class="mini">What I did</h4>
        <ul class="bullets">
          <li>Partnered with customers and engineers to capture requirements and translate them into delivery-ready artifacts (vision notes, feature briefs, release checklists).</li>
          <li>Supported UI updates by clarifying scope, acceptance criteria, and release readiness; ensured changes aligned with stakeholder expectations.</li>
          <li>Built and maintained project plans (Gantt-style schedules), risk logs, and stakeholder presentations; communicated progress and coordinated corrective actions.</li>
        </ul>

        <h4 class="mini">Impact</h4>
        <ul class="bullets">
          <li>Coordinated <b>~8–15</b> requirements discussions and produced <b>~10–25</b> delivery artifacts (briefs/checklists) to reduce ambiguity and improve handoffs.</li>
          <li>Maintained a delivery plan across <b>~2–4</b> parallel workstreams, tracking <b>~15–40</b> tasks and dependencies with weekly progress reporting.</li>
          <li>Logged and managed <b>~5–12</b> delivery risks/issues; enabled faster corrective action and improved on-time delivery confidence.</li>
        </ul>

        <div class="tech">
          <span class="tag">Requirements Gathering</span>
          <span class="tag">Feature Briefs</span>
          <span class="tag">Release Checklists</span>
          <span class="tag">Gantt Planning</span>
          <span class="tag">Risk Logs</span>
          <span class="tag">Stakeholder Updates</span>
          <span class="tag">Agile Delivery</span>
        </div>
      </div>

      </div>
      <div class="right">Bengaluru, India</div>
    </div>
  
     <div class="row">
      <div>
        <div class="title">Data Science Research Intern• Shiash Info Solutions <Pvt class="Ltd"></Pvt></div>
        <div class="meta">Jun 2023 – Oct 2023</div>
        <div class="sub">
           Built ETL-style pipelines in SQL/Python to clean, normalize, and merge 300K+ rows of business data, then ran EDA + modeling to surface trend drivers and improve baseline model performance.
        </div>
     
        <h4 class="mini">What I did</h4>
        <ul class="bullets">
          <li>Built ETL pipelines to clean and merge multi-source datasets (300K+ rows) using normalization, categorical handling, and consistent typing; reduced manual cleanup by ~5 hrs/week.</li>
          <li>Performed deep EDA to identify trend drivers, anomalies, and segment behavior; translated findings into actionable insights for stakeholders.</li>
          <li>Validated baseline regression/classification models; improved F1 by 9% through feature refinement and decision-threshold tuning with stakeholder-ready tables/visuals.</li>
        </ul>

        <h4 class="mini">Results</h4>
        <ul class="bullets">
          <li>Processed and standardized 300K+ rows across multiple sources into an analytics-ready dataset.</li>
          <li>Cut manual data cleanup by ~5 hours/week through reusable ETL + validation checks.</li>
          <li>Improved model F1 by 9% through feature engineering and threshold optimization.</li>
        </ul>

        <div class="tech">
          <span class="tag">SQL</span>
          <span class="tag">Python</span>
          <span class="tag">pandas</span>
          <span class="tag">NumPy</span>
          <span class="tag">Git</span>
          <span class="tag">EDA</span>
          <span class="tag">Model Evaluation</span>
        </div>
      </div>
      <div class="right">Hyderabad, India</div>
    </div>

  <div class="row">
  <div>
    <div class="title">AWS Software Engineering Intern • Data Automation</div>
    <div class="meta">Choice Solutions Pvt. Ltd • Feb 2022 – May 2022</div>

    <div class="sub">
      Automated data extraction and reporting refresh workflows with repeatable scripts and structured logging, improving reliability of updates and speeding up issue triage with clear runbooks.
    </div>

    <h4 class="mini">What I did</h4>
    <ul class="bullets">
      <li>Automated data refresh workflows for extraction and reporting using repeatable scripts and consistent execution patterns.</li>
      <li>Supported file-to-table style loads, schema alignment, and environment-to-environment data transfers to reduce manual steps and avoid drift.</li>
      <li>Created runbooks and troubleshooting guides so teams could operate and maintain jobs reliably.</li>
    </ul>

    <h4 class="mini">Results</h4>
    <ul class="bullets">
      <li>Improved reliability of scheduled refreshes by standardizing execution and adding structured logs.</li>
      <li>Reduced issue triage time by enabling faster debugging through consistent logging and runbook-driven steps.</li>
      <li>Improved handoff and maintainability by documenting workflows and common failure modes.</li>
    </ul>

    <div class="tech">
      <span class="tag">AWS</span>
      <span class="tag">EC2</span>
      <span class="tag">S3</span>
      <span class="tag">Lambda</span>
      <span class="tag">API Gateway</span>
      <span class="tag">Python</span>
      <span class="tag">SQL</span>
      <span class="tag">Git</span>
      <span class="tag">Logging</span>
    </div>
  </div>

  <div class="right">India</div>
</div>

     <div class="row">
      <div>
        <div class="title">Software Engineer Intern • Brainovision Solutions India Pvt.Ltd</div>
        <div class="meta">May 2021 – Jun 2021</div>
        <div class="sub">
            Automated data extraction and reporting refreshes with repeatable scripts and structured logging, improving reliability of data updates and speeding up issue triage with clear runbooks.
          </div>

          <h4 class="mini">What I did</h4>
          <ul class="bullets">
            <li>Automated extraction and reporting refresh workflows with repeatable scripts, improving reliability through structured logging and consistent run execution.</li>
            <li>Supported file-to-table loads, schema alignment, and environment-to-environment transfers to reduce manual steps and avoid schema drift.</li>
            <li>Authored runbooks and troubleshooting steps to enable reliable handoffs and faster maintenance by other teams.</li>
          </ul>

          <h4 class="mini">Results</h4>
          <ul class="bullets">
            <li>Improved repeatability of data refreshes with scripted runs and structured logs.</li>
            <li>Accelerated issue triage using standardized logging and runbook-driven troubleshooting.</li>
            <li>Enabled smoother environment migrations via schema alignment and load validation checks.</li>
          </ul>

          <div class="tech">
            <span class="tag">AWS</span>
            <span class="tag">EC2</span>
            <span class="tag">S3</span>
            <span class="tag">Lambda</span>
            <span class="tag">API Gateway</span>
            <span class="tag">Python</span>
            <span class="tag">SQL</span>
            <span class="tag">Linux</span>
            <span class="tag">Git</span>
            <span class="tag">Logging</span>
          </div>
        </div>

      <div class="right">Hyderabad, India</div>
    </div>

  </div>
</section>


    <section id="contact" class="card">
      <h2>Contact</h2>
      <p>
        Email: <a href="mailto:venkatasivareddy003@gmail.com">venkatasivareddy003@gmail.com</a><br/>
        GitHub: <a href="https://github.com/venkatasivanaga" target="_blank" rel="noreferrer">venkatasivanaga</a>
      </p>
    </section>
  </main>

  <footer class="container footer">
    <p>© <span id="y"></span> Venkata Siva Reddy Naga</p>
  </footer>

  <script>
    document.getElementById("y").textContent = new Date().getFullYear();
  </script>
</body>
</html>
